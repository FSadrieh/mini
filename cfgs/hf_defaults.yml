learning_rate: 5e-5
batch_size: 32
lr_schedule: linear
warmup_period: 0
beta2: 0.999
weight_decay: 0.0
micro_batch_size: 8
# gradient_accumulation_steps: 1
eval_micro_batch_size: 64
eval_samples: 10000
